MIT launches $27m plan to advance AI' research for good
Scientists have teamed up in a $27 million global initiative to ensure that artificial intelligence doesn’t turn against humanity.

The MIT Media Lab and the Berkman Klein Center for Internet and Society at Harvard University are spearheading the effort, serving as the founding institutions.

As AI becomes increasingly integral in our everyday lives, the researchers say it is necessary to bridge the gap between humans and machines, while preventing the ‘plagues’ of society from being adopted in the process.

Scroll down for video

Scientists have teamed up in a $27 million global initiative to ensure that artificial intelligence doesn’t turn against humanity. The MIT Media Lab and the Berkman Klein Center for Internet and Society at Harvard University are spearheading the effort

The initiative, known as the Ethics and Governance of Artificial Intelligence Fund, is backed with $27 million from the Knight Foundation, LinkedIn co-founder Reid Hoffman, the Omidyar Network, the William and Flora Hewlett Foundation, and Jim Pallotta, founder of the Raptor Group.

It will attempt to facilitate global research on AI for public interest, applied research, and education, and improve the public’s understanding of AI.

According to Alberto Ibargüen, president and CEO of the John S. and James L. Knight Foundation, ‘artificial intelligence agents will impact every part of our lives in every society on Earth,’ making initiatives such as this important as development moves forward.

The initiative aims to create a cross-disciplinary mechanism to help tackle the complex issues that are just beginning to emerge.

‘AI’s rapid development brings along a lot of tough challenges,’ explains Joi Ito, director of the MIT Media Lab.

‘For example, one of the most critical challenges is how do we make sure that the machines we ‘train’ don’t perpetuate and amplify the same human biases that plague society?

‘How can we best initiate a broader, in-depth discussion about how society will co-evolve with this technology, and connect computer science and social sciences to develop intelligent machines that are not only ‘smart’ but also socially responsible?’

TEACHING MORALS TO SELF-DRIVING CARS Researchers at the Massachusetts Institute of Technology are asking people worldwide how they think a robot car should handle such life-or-death decisions. Their goal is not just for better algorithms and ethical tenets to guide autonomous vehicles, but to understand what it will take for society to accept the vehicles and use them. Click here to play the full game The site shows players a series of 13 scenarios, highlighting two possible actions. It reveals the number of people killed by each option, including their background. Other options include saving or sacrificing dogs, pedestrians and nearby homeless people. Their findings present a dilemma for car makers and governments eager to introduce self-driving vehicles on the promise that they'll be safer than human-controlled cars. People prefer a self-driving car to act in the greater good, sacrificing its passenger if it can save a crowd of pedestrians. They just don't want to get into that car. Preliminary, unpublished research based on millions of responses from more than 160 countries shows broad differences between East and West. More prominent in the United States and Europe are judgments that reflect the utilitarian principle of minimizing total harm over all else, Rahwan said. Source: Associated Press

The initiative will be phased in over the next few years, working with efforts that are already in existence today.

The fund will also be used for an AI fellowship program along with a number of other programs, including a ‘brain trust’ of experts in the field.

Researchers are already working to take on some of the challenges AI will present, like the moral questions regarding self-driving vehicles.

‘AI could be as big a disruptor to the world as the Industrial Revolution was in the 18th and 19th centuries,’ says Iyad Rahwan, who leads the Scalable Cooperation group.

‘What we need is something that puts our entire society in the control loop of these systems … technologists, engineers, the public, ethicists, cognitive scientists, economists, legal scholars, anthropologists, faith leaders, government regulators – everyone crucial to protecting the public interest.’

As AI becomes increasingly integral in our everyday lives, the researchers say it is necessary to bridge the gap between humans and machines, while preventing the ‘plagues’ of society from being learned in the process

While there are clear benefits to artificial intelligence, Cynthia Breazeal, who leads the Personal Robots group, notes that it is also ‘a kind of double-edged sword.’

‘What should it be learning and adapting to benefit?’ Breazeal said. ‘And what should it do to protect your privacy and your security?’

The Ethics and Governance of Artificial Intellifence Fund will be governed by a small board, with leaders from each participating institution, according to the MIT Media Lab.

WILL AI STEAL YOUR JOB? The White House recently published a report warning millions of American may lose their jobs to AI. Less-skilled and less-educated workers will be affected the most. The technology may also increase wealth inequality. However, AI will improve the country's productivity growth, which would result in higher wages and fewer work hours. New jobs will also be created for those who were replaced for robots. Humans will still be needed to engage with consumers, develop AI and supervisor tasks and the technology.

There will also be a number of selected advisors, including Daniela Rus, Andrew and Erna Viterbi Professor Electrical Engineering and director of MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) along with Max Tegmark, professor of physics at MIT and co-founder of the Future of Life Institute.

‘The threat running through these otherwise disparate phenomena is a shift of reasoning and judgment away from people,’ says Jonathan Zittrain, co-founder of the Berkman Klein Center and professor of law and computer science at Harvard University.

‘Sometimes that’s good, as it can free us up for other pursuits and for deeper undertakings. And sometimes it’s profoundly worrisome, as it decouples big decisions from human understanding and accountability.

‘A lot of our work in this area will be to identify and cultivate technologies and practices that promote human autonomy and dignity rather than diminish it.’